### 10.3 In the regression model $y = \beta_1+ \beta_2x + e$, assume $x$ is endogenous and that $z$ is a valid instrument. In Section 10.3.5, we saw that $\beta_2 = cov(z, y)∕cov(z, x)$.
---

#### a. Divide the denominator of $\beta_2 = cov(z, y)∕cov(z, x)$ by $var(z)$. Show that $cov(z, x)/var(z)$ is the coefficient of the simple regression with dependent variable $x$ and explanatory variable $z$, $x = \gamma_1 + \theta_1z + v$. [Hint: See Section 10.2.1.] Note that this is the first-stage equation in two-stage least squares.

Subtract $E(x) = \gamma_1 + \theta_1 E(z)$ from $x = \gamma_1 + \theta_1 z + \nu$ to obtain

$$
x - E(x) = \theta_1 (z - E(z)) + \nu
$$

Multiply both sides by $(z - E(z))$ to obtain

$$
(z - E(z))(x - E(x)) = \theta_1 (z - E(z))^2 + (z - E(z))\nu
$$

Take the expected value of both sides to obtain

$$
E[(z - E(z))(x - E(x))] = \theta_1 E[(z - E(z))^2] + E[(z - E(z))\nu] = \theta_1 E[(z - E(z))^2]
$$

assuming $E[(z - E(z))\nu] = 0$. Solving for $\theta_1$, we obtain

$$
\theta_1 = \frac{E[(z - E(z))(x - E(x))]}{E[(z - E(z))^2]} = \frac{\text{cov}(z, x)}{\text{var}(z)}
$$

This is the OLS estimator of $\theta_1$ in the regression $x = \gamma_1 + \theta_1 z + \nu$.

#### b. Divide the numerator of $\beta_2 = cov(z, y)∕cov(z, x)$ by $var(z)$. Show that $cov(z, y)/var(z)$ is the coefficient of a simple regression with dependent variable $y$ and explanatory variable $z$, $y = \pi_0 + \pi_1z + u$. [Hint: See Section 10.2.1.]

Subtract $E(y) = \pi_0 + \pi_1 E(z)$ from $y = \pi_0 + \pi_1 z + u$ to obtain

$$
y - E(y) = \pi_0 + \pi_1 (z - E(z)) + u
$$

Multiply both sides by $(z - E(z))$ to obtain

$$
(z - E(z))(y - E(y)) = \pi_1 (z - E(z))^2 + (z - E(z))u
$$

Assuming $E[(z - E(z))u] = 0$, take the expected value of both sides to obtain

$$
E[(z - E(z))(y - E(y))] = \pi_1 E[(z - E(z))^2]
$$

#### c. In the model $y = \beta_1+ \beta_2x + e$, substitute for $x$ using $x = \gamma_1 + \theta_1z + v$ and simplify to obtain $y = \pi_0 + \pi_1z + u$. What are $\pi_0, \pi_1$, and $u$ in terms of the regression model parameters and error and the first-stage parameters and error? The regression you have obtained is a reduced-form equation.

Start with the structural equation: $y = \beta_1 + \beta_2 x + e$

Substitute the first-stage regression: $x = \gamma_1 + \theta_1 z + v$

Then: $y = \beta_1 + \beta_2 (\gamma_1 + \theta_1 z + v) + e = \beta_1 + \beta_2 \gamma_1 + \beta_2 \theta_1 z + \beta_2 v + e$

Define:

- $\pi_0 = \beta_1 + \beta_2 \gamma_1$
- $\pi_1 = \beta_2 \theta_1$
- $u = \beta_2 v + e$

Thus, we obtain the reduced-form equation: $y = \pi_0 + \pi_1 z + u$

#### d. Show that $\beta_2 = \pi_1∕ \theta_1$.

From above:

$\pi_1 = \beta_2 \theta_1 \quad \Rightarrow \quad \beta_2 = \frac{\pi_1}{\theta_1}$

This shows the IV estimator can be interpreted as the ratio of reduced-form to first-stage slope coefficients.

#### e. If $\hat{\pi_1}$ and  $\hat{\theta_1}$ are the OLS estimators of $\pi_1$ and $\theta_1$, show that $\hat{\beta_2} = \hat{\pi_1} / \hat{\theta_1}$ is a consistent estimator of $\beta_2 = \pi_1∕ \theta_1$. The estimator $\hat{\beta_2} = \hat{\pi_1} / \hat{\theta_1}$ is an indirect least squares estimator.

If $\hat{\pi}_1$ and $\hat{\theta}_1$ are the OLS estimates of $\pi_1$ and $\theta_1$, then:

$$
\hat{\beta}_2 = \frac{\hat{\pi}_1}{\hat{\theta}_1}
$$

As both estimators are consistent (z is valid and exogenous), and $\theta_1 \neq 0$, then:

$$
\hat{\beta}_2 \rightarrow \frac{\pi_1}{\theta_1} = \beta_2
$$

This proves that $\hat{\beta}_2$ is a consistent estimator of $\beta_2$, known as the **Indirect Least Squares (ILS)** estimator.
