## C10Q03

Problem 10.3 (Instrumental Variables and Indirect Least Squares)

In the regression model:

$$
y = \beta_1 + \beta_2 x + e
$$

assume that $x$ is endogenous and that $z$ is a valid instrument.

From Section 10.3.5, we know that:

$$
\beta_2 = \frac{\text{Cov}(z, y)}{\text{Cov}(z, x)}
$$

Answer the following:

**(a)**
Divide the denominator of $\beta_2 = \frac{\text{Cov}(z, y)}{\text{Cov}(z, x)}$ by $\text{Var}(z)$, and show that:

$$
\frac{\text{Cov}(z, x)}{\text{Var}(z)}
$$

is the coefficient from a simple linear regression of $x$ on $z$:

$$
x = \gamma_1 + \theta_1 z + \nu
$$

> 🔎 *Hint: See Section 10.2.1. This is the first-stage regression in two-stage least squares.*


**(b)**
Divide the numerator of $\beta_2 = \frac{\text{Cov}(z, y)}{\text{Cov}(z, x)}$ by $\text{Var}(z)$, and show that:

$$
\frac{\text{Cov}(z, y)}{\text{Var}(z)}
$$

is the coefficient from a simple regression of $y$ on $z$:

$$
y = \pi_0 + \pi_1 z + u
$$

> 🔎 *Hint: See Section 10.2.1.*


**(c)**
In the model:

$$
y = \beta_1 + \beta_2 x + e
$$

substitute for $x$ using:

$$
x = \gamma_1 + \theta_1 z + \nu
$$

and simplify to obtain:

$$
y = \pi_0 + \pi_1 z + u
$$

Express $\pi_0$, $\pi_1$, and $u$ in terms of the regression model parameters and the error terms. The equation you obtain is the **reduced-form equation**.


**(d)**
Show that:

$$
\beta_2 = \frac{\pi_1}{\theta_1}
$$


**(e)**
If $\hat{\pi}_1$ and $\hat{\theta}_1$ are the OLS estimators of $\pi_1$ and $\theta_1$, show that:

$$
\hat{\beta}_2 = \frac{\hat{\pi}_1}{\hat{\theta}_1}
$$

is a consistent estimator for $\beta_2$.

This estimator is called the **indirect least squares (ILS)** estimator.


----

### ANS.

在回歸模型：

$$
y = \beta_1 + \beta_2 x + e
$$

中，假設 $x$ 是內生變數，z 是一個有效的工具變數。

我們知道：

$$
\beta_2 = \frac{\text{Cov}(z, y)}{\text{Cov}(z, x)}
$$

**(a)** 證明 $\frac{\text{Cov}(z, x)}{\text{Var}(z)}$ 是一個回歸係數

考慮簡單線性回歸：

$$
x = \gamma_1 + \theta_1 z + \nu
$$


對兩邊取期望：

   $$
   E(x) = \gamma_1 + \theta_1 E(z)
   $$

兩邊減去期望：

   $$
   x - E(x) = \theta_1 (z - E(z)) + \nu
   $$

將等式兩邊都乘上 $z - E(z)$：

   $$
   (z - E(z))(x - E(x)) = \theta_1 (z - E(z))^2 + \nu (z - E(z))
   $$

對整個式子取期望值：

   $$
   E\left[(z - E(z))(x - E(x))\right] = \theta_1 E\left[(z - E(z))^2\right] + E\left[\nu (z - E(z))\right]
   $$

假設 $\nu$ 與 $z$ 無關（工具變數的基本假設），則右邊的誤差項期望為 0：

   $$
   E[\nu(z - E(z))] = 0
   $$

   所以得：

   $$
   E\left[(z - E(z))(x - E(x))\right] = \theta_1 E\left[(z - E(z))^2\right]
   $$

解出 $\theta_1$：**

   $$
   \theta_1 = \frac{E[(z - E(z))(x - E(x))]}{E[(z - E(z))^2]} = \frac{\text{Cov}(z, x)}{\text{Var}(z)}
   $$

------

**(b)** 證明 $\frac{\text{Cov}(z, y)}{\text{Var}(z)}$ 是另一個回歸係數

考慮簡單線性回歸：

$$
y = \pi_0 + \pi_1 z + u
$$


對兩邊取期望值

$$
E(y) = \pi_0 + \pi_1 E(z)
$$


兩邊減去期望

$$
y - E(y) = \pi_1 (z - E(z)) + u
$$


兩邊乘上 $z - E(z)$

$$
(z - E(z))(y - E(y)) = \pi_1 (z - E(z))^2 + u (z - E(z))
$$


對整個式子取期望值：

$$
E[(z - E(z))(y - E(y))] = \pi_1 E[(z - E(z))^2] + E[u (z - E(z))]
$$


假設 $u$ 與 $z$ 無關（IV 基本假設）

$$
E[u (z - E(z))] = 0
$$

因此有：

$$
E[(z - E(z))(y - E(y))] = \pi_1 E[(z - E(z))^2]
$$


解出 $\pi_1$：

$$
\pi_1 = \frac{E[(z - E(z))(y - E(y))]}{E[(z - E(z))^2]} = \frac{\text{Cov}(z, y)}{\text{Var}(z)}
$$

------

**(c)** 推導 reduced-form 方程式

從原始模型：

$$
y = \beta_1 + \beta_2 x + e
$$

將第一階段回歸 $x = \gamma_1 + \theta_1 z + \nu$ 代入得：

$$
y = \beta_1 + \beta_2 (\gamma_1 + \theta_1 z + \nu) + e \\
= \beta_1 + \beta_2 \gamma_1 + \beta_2 \theta_1 z + \beta_2 \nu + e
$$

令：

- $\pi_0 = \beta_1 + \beta_2 \gamma_1$
- $\pi_1 = \beta_2 \theta_1$
- $u = \beta_2 \nu + e$

得到：

$$
y = \pi_0 + \pi_1 z + u
$$

這是 reduced-form 模型。

-----

**(d)** 證明 $\beta_2 = \frac{\pi_1}{\theta_1}$

由上一步可得：

$$
\pi_1 = \beta_2 \theta_1
\quad \Rightarrow \quad
\beta_2 = \frac{\pi_1}{\theta_1}
$$

-----

**(e)** 證明間接最小平方法估計量為一致估計量

推導 $\theta_1$

$$
\theta_1 = \frac{E[(z - E(z))(x - E(x))]}{E[(z - E(z))^2]} = \frac{\text{Cov}(z, x)}{\text{Var}(z)}
$$

$$
\hat{\theta}_1 = \frac{\widehat{\text{Cov}}(z, x)}{\widehat{\text{Var}}(z)} 
= \frac{\sum (z_i - \bar{z})(x_i - \bar{x}) / N}{\sum (z_i - \bar{z})^2 / N} 
= \frac{\sum (z_i - \bar{z})(x_i - \bar{x})}{\sum (z_i - \bar{z})^2}
$$

> This estimator is consistent if $z$ is uncorrelated with $v$.


推導 $\pi_1$

$$
\pi_1 = \frac{E[(z - E(z))(y - E(y))]}{E[(z - E(z))^2]} = \frac{\text{Cov}(z, y)}{\text{Var}(z)}
$$

$$
\hat{\pi}_1 = \frac{\widehat{\text{Cov}}(z, y)}{\widehat{\text{Var}}(z)} 
= \frac{\sum (z_i - \bar{z})(y_i - \bar{y}) / N}{\sum (z_i - \bar{z})^2 / N} 
= \frac{\sum (z_i - \bar{z})(y_i - \bar{y})}{\sum (z_i - \bar{z})^2}
$$

> This estimator is consistent if $z$ is uncorrelated with $u$.


間接估計 $\beta_2$

$$
\beta_2 = \frac{\pi_1}{\theta_1}
$$

$$
\hat{\beta}_2 = \frac{\hat{\pi}_1}{\hat{\theta}_1}
= \frac{\left[ \frac{\sum (z_i - \bar{z})(y_i - \bar{y})}{\sum (z_i - \bar{z})^2} \right]}
       {\left[ \frac{\sum (z_i - \bar{z})(x_i - \bar{x})}{\sum (z_i - \bar{z})^2} \right]}
= \frac{\sum (z_i - \bar{z})(y_i - \bar{y})}{\sum (z_i - \bar{z})(x_i - \bar{x})}
$$


大樣本極限定理

當 $N \to \infty$ 時，有：

$$
\widehat{\text{Cov}}(z, y) \overset{p}{\to} \text{Cov}(z, y), \quad 
\widehat{\text{Cov}}(z, x) \overset{p}{\to} \text{Cov}(z, x)
$$

因此：

$$
\hat{\beta}_2 = \frac{\widehat{\text{Cov}}(z, y)}{\widehat{\text{Cov}}(z, x)} 
\overset{p}{\to} \beta_2 = \frac{\text{Cov}(z, y)}{\text{Cov}(z, x)}
$$

---







