## C10Q03

Problem 10.3 (Instrumental Variables and Indirect Least Squares)

In the regression model:

$$
y = \beta_1 + \beta_2 x + e
$$

assume that $x$ is endogenous and that $z$ is a valid instrument.

From Section 10.3.5, we know that:

$$
\beta_2 = \frac{\text{Cov}(z, y)}{\text{Cov}(z, x)}
$$

Answer the following:

**(a)**
Divide the denominator of $\beta_2 = \frac{\text{Cov}(z, y)}{\text{Cov}(z, x)}$ by $\text{Var}(z)$, and show that:

$$
\frac{\text{Cov}(z, x)}{\text{Var}(z)}
$$

is the coefficient from a simple linear regression of $x$ on $z$:

$$
x = \gamma_1 + \theta_1 z + \nu
$$

> ðŸ”Ž *Hint: See Section 10.2.1. This is the first-stage regression in two-stage least squares.*


**(b)**
Divide the numerator of $\beta_2 = \frac{\text{Cov}(z, y)}{\text{Cov}(z, x)}$ by $\text{Var}(z)$, and show that:

$$
\frac{\text{Cov}(z, y)}{\text{Var}(z)}
$$

is the coefficient from a simple regression of $y$ on $z$:

$$
y = \pi_0 + \pi_1 z + u
$$

> ðŸ”Ž *Hint: See Section 10.2.1.*


**(c)**
In the model:

$$
y = \beta_1 + \beta_2 x + e
$$

substitute for $x$ using:

$$
x = \gamma_1 + \theta_1 z + \nu
$$

and simplify to obtain:

$$
y = \pi_0 + \pi_1 z + u
$$

Express $\pi_0$, $\pi_1$, and $u$ in terms of the regression model parameters and the error terms. The equation you obtain is the **reduced-form equation**.


**(d)**
Show that:

$$
\beta_2 = \frac{\pi_1}{\theta_1}
$$


**(e)**
If $\hat{\pi}_1$ and $\hat{\theta}_1$ are the OLS estimators of $\pi_1$ and $\theta_1$, show that:

$$
\hat{\beta}_2 = \frac{\hat{\pi}_1}{\hat{\theta}_1}
$$

is a consistent estimator for $\beta_2$.

This estimator is called the **indirect least squares (ILS)** estimator.


----

### ANS.

Problem Context

In the regression model:

$$
y = \beta_1 + \beta_2 x + e
$$

assume that $x$ is endogenous and $z$ is a valid instrument.

It is known that:

$$
\beta_2 = \frac{\text{Cov}(z, y)}{\text{Cov}(z, x)}
$$



**(a)** Show that $\frac{\text{Cov}(z, x)}{\text{Var}(z)}$ is a regression coefficient

Consider the simple linear regression:

$$
x = \gamma_1 + \theta_1 z + \nu
$$

The OLS estimator is:

$$
\hat{\theta}_1 = \frac{\text{Cov}(z, x)}{\text{Var}(z)}
$$

Therefore, $\frac{\text{Cov}(z, x)}{\text{Var}(z)} = \theta_1$

This is the **first-stage** regression in two-stage least squares (2SLS).



**(b)** Show that $\frac{\text{Cov}(z, y)}{\text{Var}(z)}$ is a regression coefficient

Consider another simple regression:

$$
y = \pi_0 + \pi_1 z + u
$$

The OLS estimator is:

$$
\hat{\pi}_1 = \frac{\text{Cov}(z, y)}{\text{Var}(z)}
$$

Thus, $\frac{\text{Cov}(z, y)}{\text{Var}(z)} = \pi_1$

This is the **reduced-form** regression of $y$ on $z$.



**(c)** Derive the reduced-form equation

Start with the structural model:

$$
y = \beta_1 + \beta_2 x + e
$$

Substitute in the first-stage regression: $x = \gamma_1 + \theta_1 z + \nu$:

$$
y = \beta_1 + \beta_2 (\gamma_1 + \theta_1 z + \nu) + e \\
= \beta_1 + \beta_2 \gamma_1 + \beta_2 \theta_1 z + \beta_2 \nu + e
$$

Define:

- $\pi_0 = \beta_1 + \beta_2 \gamma_1$
- $\pi_1 = \beta_2 \theta_1$
- $u = \beta_2 \nu + e$

We get the reduced-form equation:

$$
y = \pi_0 + \pi_1 z + u
$$



**(d)** Show that $\beta_2 = \frac{\pi_1}{\theta_1}$

From (c), we know:

$$
\pi_1 = \beta_2 \theta_1 \\
\Rightarrow \beta_2 = \frac{\pi_1}{\theta_1}
$$



**(e)** Show that $\hat{\beta}_2 = \frac{\hat{\pi}_1}{\hat{\theta}_1}$ is a consistent estimator

If $\hat{\pi}_1$ and $\hat{\theta}_1$ are consistent OLS estimators, then:

$$
\hat{\beta}_2 = \frac{\hat{\pi}_1}{\hat{\theta}_1}
$$

Since:

- $\hat{\pi}_1 \xrightarrow{p} \pi_1$
- $\hat{\theta}_1 \xrightarrow{p} \theta_1$

By the **Continuous Mapping Theorem**:

$$
\hat{\beta}_2 = \frac{\hat{\pi}_1}{\hat{\theta}_1} \xrightarrow{p} \frac{\pi_1}{\theta_1} = \beta_2
$$

Hence, $\hat{\beta}_2$ is a consistent estimator of $\beta_2$, known as the **indirect least squares (ILS)** estimator.

---







