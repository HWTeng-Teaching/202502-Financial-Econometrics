## C10Q03

Problem 10.3 (Instrumental Variables and Indirect Least Squares)

In the regression model:

$$
y = \beta_1 + \beta_2 x + e
$$

assume that $x$ is endogenous and that $z$ is a valid instrument.

From Section 10.3.5, we know that:

$$
\beta_2 = \frac{\text{Cov}(z, y)}{\text{Cov}(z, x)}
$$

Answer the following:

**(a)**
Divide the denominator of $\beta_2 = \frac{\text{Cov}(z, y)}{\text{Cov}(z, x)}$ by $\text{Var}(z)$, and show that:

$$
\frac{\text{Cov}(z, x)}{\text{Var}(z)}
$$

is the coefficient from a simple linear regression of $x$ on $z$:

$$
x = \gamma_1 + \theta_1 z + \nu
$$

> ğŸ” *Hint: See Section 10.2.1. This is the first-stage regression in two-stage least squares.*


**(b)**
Divide the numerator of $\beta_2 = \frac{\text{Cov}(z, y)}{\text{Cov}(z, x)}$ by $\text{Var}(z)$, and show that:

$$
\frac{\text{Cov}(z, y)}{\text{Var}(z)}
$$

is the coefficient from a simple regression of $y$ on $z$:

$$
y = \pi_0 + \pi_1 z + u
$$

> ğŸ” *Hint: See Section 10.2.1.*


**(c)**
In the model:

$$
y = \beta_1 + \beta_2 x + e
$$

substitute for $x$ using:

$$
x = \gamma_1 + \theta_1 z + \nu
$$

and simplify to obtain:

$$
y = \pi_0 + \pi_1 z + u
$$

Express $\pi_0$, $\pi_1$, and $u$ in terms of the regression model parameters and the error terms. The equation you obtain is the **reduced-form equation**.


**(d)**
Show that:

$$
\beta_2 = \frac{\pi_1}{\theta_1}
$$


**(e)**
If $\hat{\pi}_1$ and $\hat{\theta}_1$ are the OLS estimators of $\pi_1$ and $\theta_1$, show that:

$$
\hat{\beta}_2 = \frac{\hat{\pi}_1}{\hat{\theta}_1}
$$

is a consistent estimator for $\beta_2$.

This estimator is called the **indirect least squares (ILS)** estimator.


----

### ANS.

åœ¨å›æ­¸æ¨¡å‹ï¼š

$$
y = \beta_1 + \beta_2 x + e
$$

ä¸­ï¼Œå‡è¨­ $x$ æ˜¯å…§ç”Ÿè®Šæ•¸ï¼Œz æ˜¯ä¸€å€‹æœ‰æ•ˆçš„å·¥å…·è®Šæ•¸ã€‚

æˆ‘å€‘çŸ¥é“ï¼š

$$
\beta_2 = \frac{\text{Cov}(z, y)}{\text{Cov}(z, x)}
$$

**(a)** è­‰æ˜ $\frac{\text{Cov}(z, x)}{\text{Var}(z)}$ æ˜¯ä¸€å€‹å›æ­¸ä¿‚æ•¸

è€ƒæ…®ç°¡å–®ç·šæ€§å›æ­¸ï¼š

$$
x = \gamma_1 + \theta_1 z + \nu
$$


å°å…©é‚Šå–æœŸæœ›ï¼š

   $$
   E(x) = \gamma_1 + \theta_1 E(z)
   $$

å…©é‚Šæ¸›å»æœŸæœ›ï¼š

   $$
   x - E(x) = \theta_1 (z - E(z)) + \nu
   $$

å°‡ç­‰å¼å…©é‚Šéƒ½ä¹˜ä¸Š $z - E(z)$ï¼š

   $$
   (z - E(z))(x - E(x)) = \theta_1 (z - E(z))^2 + \nu (z - E(z))
   $$

å°æ•´å€‹å¼å­å–æœŸæœ›å€¼ï¼š

   $$
   E\left[(z - E(z))(x - E(x))\right] = \theta_1 E\left[(z - E(z))^2\right] + E\left[\nu (z - E(z))\right]
   $$

å‡è¨­ $\nu$ èˆ‡ $z$ ç„¡é—œï¼ˆå·¥å…·è®Šæ•¸çš„åŸºæœ¬å‡è¨­ï¼‰ï¼Œå‰‡å³é‚Šçš„èª¤å·®é …æœŸæœ›ç‚º 0ï¼š

   $$
   E[\nu(z - E(z))] = 0
   $$

   æ‰€ä»¥å¾—ï¼š

   $$
   E\left[(z - E(z))(x - E(x))\right] = \theta_1 E\left[(z - E(z))^2\right]
   $$

è§£å‡º $\theta_1$ï¼š**

   $$
   \theta_1 = \frac{E[(z - E(z))(x - E(x))]}{E[(z - E(z))^2]} = \frac{\text{Cov}(z, x)}{\text{Var}(z)}
   $$

------

**(b)** è­‰æ˜ $\frac{\text{Cov}(z, y)}{\text{Var}(z)}$ æ˜¯å¦ä¸€å€‹å›æ­¸ä¿‚æ•¸

è€ƒæ…®ç°¡å–®ç·šæ€§å›æ­¸ï¼š

$$
y = \pi_0 + \pi_1 z + u
$$


å°å…©é‚Šå–æœŸæœ›å€¼

$$
E(y) = \pi_0 + \pi_1 E(z)
$$


å…©é‚Šæ¸›å»æœŸæœ›

$$
y - E(y) = \pi_1 (z - E(z)) + u
$$


å…©é‚Šä¹˜ä¸Š $z - E(z)$

$$
(z - E(z))(y - E(y)) = \pi_1 (z - E(z))^2 + u (z - E(z))
$$


å°æ•´å€‹å¼å­å–æœŸæœ›å€¼ï¼š

$$
E[(z - E(z))(y - E(y))] = \pi_1 E[(z - E(z))^2] + E[u (z - E(z))]
$$


å‡è¨­ $u$ èˆ‡ $z$ ç„¡é—œï¼ˆIV åŸºæœ¬å‡è¨­ï¼‰

$$
E[u (z - E(z))] = 0
$$

å› æ­¤æœ‰ï¼š

$$
E[(z - E(z))(y - E(y))] = \pi_1 E[(z - E(z))^2]
$$


è§£å‡º $\pi_1$ï¼š

$$
\pi_1 = \frac{E[(z - E(z))(y - E(y))]}{E[(z - E(z))^2]} = \frac{\text{Cov}(z, y)}{\text{Var}(z)}
$$

------

**(c)** æ¨å° reduced-form æ–¹ç¨‹å¼

å¾åŸå§‹æ¨¡å‹ï¼š

$$
y = \beta_1 + \beta_2 x + e
$$

å°‡ç¬¬ä¸€éšæ®µå›æ­¸ $x = \gamma_1 + \theta_1 z + \nu$ ä»£å…¥å¾—ï¼š

$$
y = \beta_1 + \beta_2 (\gamma_1 + \theta_1 z + \nu) + e \\
= \beta_1 + \beta_2 \gamma_1 + \beta_2 \theta_1 z + \beta_2 \nu + e
$$

ä»¤ï¼š

- $\pi_0 = \beta_1 + \beta_2 \gamma_1$
- $\pi_1 = \beta_2 \theta_1$
- $u = \beta_2 \nu + e$

å¾—åˆ°ï¼š

$$
y = \pi_0 + \pi_1 z + u
$$

é€™æ˜¯ reduced-form æ¨¡å‹ã€‚

-----

**(d)** è­‰æ˜ $\beta_2 = \frac{\pi_1}{\theta_1}$

ç”±ä¸Šä¸€æ­¥å¯å¾—ï¼š

$$
\pi_1 = \beta_2 \theta_1
\quad \Rightarrow \quad
\beta_2 = \frac{\pi_1}{\theta_1}
$$

-----

**(e)** è­‰æ˜é–“æ¥æœ€å°å¹³æ–¹æ³•ä¼°è¨ˆé‡ç‚ºä¸€è‡´ä¼°è¨ˆé‡

æ¨å° $\theta_1$

$$
\theta_1 = \frac{E[(z - E(z))(x - E(x))]}{E[(z - E(z))^2]} = \frac{\text{Cov}(z, x)}{\text{Var}(z)}
$$

$$
\hat{\theta}_1 = \frac{\widehat{\text{Cov}}(z, x)}{\widehat{\text{Var}}(z)} 
= \frac{\sum (z_i - \bar{z})(x_i - \bar{x}) / N}{\sum (z_i - \bar{z})^2 / N} 
= \frac{\sum (z_i - \bar{z})(x_i - \bar{x})}{\sum (z_i - \bar{z})^2}
$$

> This estimator is consistent if $z$ is uncorrelated with $v$.


æ¨å° $\pi_1$

$$
\pi_1 = \frac{E[(z - E(z))(y - E(y))]}{E[(z - E(z))^2]} = \frac{\text{Cov}(z, y)}{\text{Var}(z)}
$$

$$
\hat{\pi}_1 = \frac{\widehat{\text{Cov}}(z, y)}{\widehat{\text{Var}}(z)} 
= \frac{\sum (z_i - \bar{z})(y_i - \bar{y}) / N}{\sum (z_i - \bar{z})^2 / N} 
= \frac{\sum (z_i - \bar{z})(y_i - \bar{y})}{\sum (z_i - \bar{z})^2}
$$

> This estimator is consistent if $z$ is uncorrelated with $u$.


é–“æ¥ä¼°è¨ˆ $\beta_2$

$$
\beta_2 = \frac{\pi_1}{\theta_1}
$$

$$
\hat{\beta}_2 = \frac{\hat{\pi}_1}{\hat{\theta}_1}
= \frac{\left[ \frac{\sum (z_i - \bar{z})(y_i - \bar{y})}{\sum (z_i - \bar{z})^2} \right]}
       {\left[ \frac{\sum (z_i - \bar{z})(x_i - \bar{x})}{\sum (z_i - \bar{z})^2} \right]}
= \frac{\sum (z_i - \bar{z})(y_i - \bar{y})}{\sum (z_i - \bar{z})(x_i - \bar{x})}
$$


å¤§æ¨£æœ¬æ¥µé™å®šç†

ç•¶ $N \to \infty$ æ™‚ï¼Œæœ‰ï¼š

$$
\widehat{\text{Cov}}(z, y) \overset{p}{\to} \text{Cov}(z, y), \quad 
\widehat{\text{Cov}}(z, x) \overset{p}{\to} \text{Cov}(z, x)
$$

å› æ­¤ï¼š

$$
\hat{\beta}_2 = \frac{\widehat{\text{Cov}}(z, y)}{\widehat{\text{Cov}}(z, x)} 
\overset{p}{\to} \beta_2 = \frac{\text{Cov}(z, y)}{\text{Cov}(z, x)}
$$

---







