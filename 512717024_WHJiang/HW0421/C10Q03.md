## C10Q03

In the regression model y = β₁ + β₂x + e, assume x is endogenous and that z is a valid instrument.

In Section 10.3.5 we saw that β₂ = cov(z, y) / cov(z, x).

a. Divide the denominator of β₂ = cov(z, y) / cov(z, x) by var(z). Show that cov(z, x)/var(z) is the coefficient of the simple regression with dependent variable x and explanatory variable z, x = γ₁ + θ₁z + v. (Hint: see Section 10.2.1.) Note that this is the first‑stage equation in two‑stage least squares.

b. Divide the numerator of β₂ = cov(z, y) / cov(z, x) by var(z). Show that cov(z, y)/var(z) is the coefficient of a simple regression with dependent variable y and explanatory variable z, y = π₀ + π₁z + u. (Hint: see Section 10.2.1.)

c. In the model y = β₁ + β₂x + e, substitute for x using x = γ₁ + θ₁z + ν and simplify to obtain y = π₀ + π₁z + u. What are π₀, π₁, u in terms of the regression‑model parameters and error and the first‑stage parameters and error? The regression you have obtained is a reduced‑form equation.

d. Show that β₂ = π₁ / θ₁.

e. If ˆπ₁ and ˆθ₁ are the OLS estimators of π₁ and θ₁, show that ˆβ₂ = ˆπ₁ / ˆθ₁ is a consistent estimator of β₂ = π₁ / θ₁. The estimator ˆβ₂ = ˆπ₁ / ˆθ₁ is an indirect least squares estimator.


----

### ANS.

Problem Context

In the regression model:

$$
y = \beta_1 + \beta_2 x + e
$$

assume that $x$ is endogenous and $z$ is a valid instrument.

It is known that:

$$
\beta_2 = \frac{\text{Cov}(z, y)}{\text{Cov}(z, x)}
$$



**(a)** Show that $\frac{\text{Cov}(z, x)}{\text{Var}(z)}$ is a regression coefficient

Consider the simple linear regression:

$$
x = \gamma_1 + \theta_1 z + \nu
$$

The OLS estimator is:

$$
\hat{\theta}_1 = \frac{\text{Cov}(z, x)}{\text{Var}(z)}
$$

Therefore, $\frac{\text{Cov}(z, x)}{\text{Var}(z)} = \theta_1$

This is the **first-stage** regression in two-stage least squares (2SLS).



**(b)** Show that $\frac{\text{Cov}(z, y)}{\text{Var}(z)}$ is a regression coefficient

Consider another simple regression:

$$
y = \pi_0 + \pi_1 z + u
$$

The OLS estimator is:

$$
\hat{\pi}_1 = \frac{\text{Cov}(z, y)}{\text{Var}(z)}
$$

Thus, $\frac{\text{Cov}(z, y)}{\text{Var}(z)} = \pi_1$

This is the **reduced-form** regression of $y$ on $z$.



**(c)** Derive the reduced-form equation

Start with the structural model:

$$
y = \beta_1 + \beta_2 x + e
$$

Substitute in the first-stage regression: $x = \gamma_1 + \theta_1 z + \nu$:

$$
y = \beta_1 + \beta_2 (\gamma_1 + \theta_1 z + \nu) + e \\
= \beta_1 + \beta_2 \gamma_1 + \beta_2 \theta_1 z + \beta_2 \nu + e
$$

Define:

- $\pi_0 = \beta_1 + \beta_2 \gamma_1$
- $\pi_1 = \beta_2 \theta_1$
- $u = \beta_2 \nu + e$

We get the reduced-form equation:

$$
y = \pi_0 + \pi_1 z + u
$$



**(d)** Show that $\beta_2 = \frac{\pi_1}{\theta_1}$

From (c), we know:

$$
\pi_1 = \beta_2 \theta_1 \\
\Rightarrow \beta_2 = \frac{\pi_1}{\theta_1}
$$



**(e)** Show that $\hat{\beta}_2 = \frac{\hat{\pi}_1}{\hat{\theta}_1}$ is a consistent estimator

If $\hat{\pi}_1$ and $\hat{\theta}_1$ are consistent OLS estimators, then:

$$
\hat{\beta}_2 = \frac{\hat{\pi}_1}{\hat{\theta}_1}
$$

Since:

- $\hat{\pi}_1 \xrightarrow{p} \pi_1$
- $\hat{\theta}_1 \xrightarrow{p} \theta_1$

By the **Continuous Mapping Theorem**:

$$
\hat{\beta}_2 = \frac{\hat{\pi}_1}{\hat{\theta}_1} \xrightarrow{p} \frac{\pi_1}{\theta_1} = \beta_2
$$

Hence, $\hat{\beta}_2$ is a consistent estimator of $\beta_2$, known as the **indirect least squares (ILS)** estimator.

---







