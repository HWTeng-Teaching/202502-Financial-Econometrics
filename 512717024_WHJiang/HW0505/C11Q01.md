## C11Q01

Our aim is to estimate the parameters of the simultaneous equations model:

$$
y_1 = \alpha_1 y_2 + e_1 \\
$$

$$
y_2 = \alpha_2 y_1 + \beta_1 x_1 + \beta_2 x_2 + e_2
$$

We assume that \( x_1 \) and \( x_2 \) are exogenous and uncorrelated with the error terms \( e_1 \) and \( e_2 \).

**(a)**
Solve the two structural equations for the reduced-form equation for \( y_2 \), that is,  

$$
\( y_2 = \pi_1 x_1 + \pi_2 x_2 + \nu_2 \). 
$$

Express the reduced-form parameters in terms of the **structural parameters**, and the reduced-form error in terms of \( e_1 \) and \( e_2 \).  
Show that \( y_2 \) is correlated with \( e_1 \).

**(b)**
Which equation parameters are consistently estimated using OLS? Explain.

**(c)**
Which parameters are "identified" in the simultaneous equations sense? Explain your reasoning.

**(d)**
To estimate the parameters of the reduced-form equation for \( y_2 \) using the **method of moments (MOM)**, the two moment equations are:

$$
\frac{1}{N} \sum x_{1i}(y_{2i} - \pi_1 x_{1i} - \pi_2 x_{2i}) = 0 \\
\frac{1}{N} \sum x_{2i}(y_{2i} - \pi_1 x_{1i} - \pi_2 x_{2i}) = 0
$$

Explain why these two moment conditions are a valid basis for obtaining consistent estimators of the reduced-form parameters.

**(e)**
Are the MOM estimators in part (d) the same as the OLS estimators?  
Form the sum of squared errors for \( y_2 = \pi_1 x_1 + \pi_2 x_2 + \nu_2 \), take derivatives, and show they yield the same equations.

**(f)**
Given:

$$
\( \sum x_{1i}^2 = 1 \)
\( \sum x_{2i}^2 = 1 \)
\( \sum x_{1i} x_{2i} = 0 \)
\( \sum x_{1i} y_{i} = 2 \)
\( \sum x_{1i} y_{2i} = 3 \)
\( \sum x_{2i} y_{i} = 3 \)
\( \sum x_{2i} y_{2i} = 4 \)
$$

Use the moment conditions from part (d) to solve for \( \hat{\pi}_1 = 3 \), \( \hat{\pi}_2 = 4 \).

**(g)**
The fitted value is:

$$
\hat{y}_2 = \hat{\pi}_1 x_1 + \hat{\pi}_2 x_2
$$

Explain why we can use the moment condition:

Σx₂ᵢ(ŷ₁ᵢ − α₁y₂ᵢ) = 0

to consistently estimate \( \alpha_1 \), and compute the IV estimate.

**(h)**
Find the 2SLS estimate of \( \alpha_1 \) by applying OLS to:

$$
y_1 = \alpha_1 \hat{y}_2 + e_1^*
$$

Compare with the answer in (g).


----

## ✅ Answer

**(a) 消去與代入法：推導 reduced form**

方程組：

$$
y_1 = \alpha_1 y_2 + e_1 \\
$$

$$
y_2 = \alpha_2 y_1 + \beta_1 x_1 + \beta_2 x_2 + e_2
$$

將方程式 (1) 代入方程式 (2) 並化簡：

$$
\begin{aligned}
y_2 &= \alpha_2 y_1 + \beta_1 x_1 + \beta_2 x_2 + e_2 \\
&= \alpha_2 (\alpha_1 y_2 + e_1) + \beta_1 x_1 + \beta_2 x_2 + e_2 \\
&= \alpha_1 \alpha_2 y_2 + \alpha_2 e_1 + \beta_1 x_1 + \beta_2 x_2 + e_2
\end{aligned}
$$

將 y_2  移至左邊：

$$
y_2 (1 - \alpha_1 \alpha_2) = \beta_1 x_1 + \beta_2 x_2 + e_2 + \alpha_2 e_1
$$

Solve for \( y_2 \):

$$
y_2 = \frac{\beta_1}{1 - \alpha_1 \alpha_2} x_1 + \frac{\beta_2}{1 - \alpha_1 \alpha_2} x_2 + \frac{e_2 + \alpha_2 e_1}{1 - \alpha_1 \alpha_2}
$$

定義：

$$
\pi_1 = \frac{\beta_1}{1 - \alpha_1 \alpha_2}, \quad 
\pi_2 = \frac{\beta_2}{1 - \alpha_1 \alpha_2}, \quad 
v_2 = \frac{e_2 + \alpha_2 e_1}{1 - \alpha_1 \alpha_2}
$$

則：

$$
y_2 = \pi_1 x_1 + \pi_2 x_2 + v_2
$$

To show the correlation:

$$
\begin{aligned}
\text{cov}(y_2, e_1 \mid \mathbf{x}) 
&= E(y_2 e_1 \mid \mathbf{x}) \\
&= E \left[ \left( \frac{\beta_1}{1 - \alpha_1 \alpha_2} x_1 + \frac{\beta_2}{1 - \alpha_1 \alpha_2} x_2 + \frac{e_2 + \alpha_2 e_1}{1 - \alpha_1 \alpha_2} \right) e_1 \,\middle|\, \mathbf{x} \right] \\
&= E \left[ \frac{\beta_1}{1 - \alpha_1 \alpha_2} x_1 e_1 \mid \mathbf{x} \right] + E \left[ \frac{\beta_2}{1 - \alpha_1 \alpha_2} x_2 e_1 \mid \mathbf{x} \right] + E \left[ \frac{e_2 + \alpha_2 e_1}{1 - \alpha_1 \alpha_2} e_1 \mid \mathbf{x} \right] \\
&= 0 + 0 + E \left[ \frac{e_2 e_1 + \alpha_2 e_1^2}{1 - \alpha_1 \alpha_2} \mid \mathbf{x} \right] \\
&= \frac{\alpha_2 \sigma_{e_1}^2}{1 - \alpha_1 \alpha_2} \quad \text{(若 } e_1 \text{ 與 } e_2 \text{ 不相關)}
\end{aligned}
$$

因此，當 α2 ≠ 0  且 α1 ≠ 0  時， y2  與 e1  相關 ⇒ 說明內生性。

我們已得：

$$
\text{cov}(y_2, e_1 \mid \mathbf{x}) = E(y_2 e_1 \mid \mathbf{x})
$$

接著，

$$
\begin{aligned}
\text{cov}(y_2, e_1 \mid \mathbf{x}) 
&= \frac{E(e_2 e_1 \mid \mathbf{x}) + \alpha_2 E(e_1^2 \mid \mathbf{x})}{1 - \alpha_1 \alpha_2} \\
&= \frac{\alpha_2}{1 - \alpha_1 \alpha_2} \sigma_1^2
\end{aligned}
$$

此共變數非零，除非：

$$
\alpha_2 = 0 \Rightarrow \text{無同時性問題 (no simultaneity)}
$$

----

**(b) OLS 可一致估計的條件**

因為原始兩個結構式的右邊都包含內生變數（如  y_1 ），所以不能直接使用 OLS。

但 reduced form 中只有外生變數  x_1, x_2 ，可以對其進行 OLS 一致估計。

----

**(c) 識別性（Identification）**

結構模型中  M = 2  個方程式。為了識別一條方程式，必須省略  M - 1 = 1  個外生變數。

- 方程 (1) 中已省略  x_1, x_2 ，是 **已識別**
- 方程 (2) 中未省略任何外生變數，**未被識別**

----

**(d) Moment Conditions**

這些 moment conditions 來自於  x  為外生變數（exogenous）的假設。因此有：

$$
E(x_{i1} v_{i1} \mid \mathbf{x}) = E(x_{i2} v_{i2} \mid \mathbf{x}) = 0
$$

根據 part (a) 的結果， y_2  的 reduced form 為：

$$
y_2 = \frac{\beta_1}{1 - \alpha_1 \alpha_2} x_1 + \frac{\beta_2}{1 - \alpha_1 \alpha_2} x_2 + \frac{e_2 + \alpha_2 e_1}{1 - \alpha_1 \alpha_2}
$$

簡記為：

$$
y_2 = \pi_1 x_1 + \pi_2 x_2 + v_2
$$

Reduced Form 誤差項與  x  無相關性證明：

我們要證明：

<img width="495" alt="d" src="https://github.com/user-attachments/assets/8fd9f1e9-f1b4-44f2-a14f-8e530680cf4f" />


這是因為  x  為外生變數，對  e_1, e_2  條件期望為 0。

因此，reduced form 的誤差項  v_2  與  x  無相關。

----

**(e) 最小平方法：條件最小平方**

略去下標  i 的情況下，平方和函數為：

$$
S(\pi_1, \pi_2 \mid \mathbf{y}, \mathbf{x}) = \sum (y_2 - \pi_1 x_1 - \pi_2 x_2)^2
$$

對參數求一階導數如下：

對 π1 偏微：

$$
\frac{\partial S(\pi_1, \pi_2 \mid \mathbf{y}, \mathbf{x})}{\partial \pi_1}
= 2 \sum (y_2 - \pi_1 x_1 - \pi_2 x_2) x_1 = 0
$$

對 π2 偏微：

$$
\frac{\partial S(\pi_1, \pi_2 \mid \mathbf{y}, \mathbf{x})}{\partial \pi_2}
= 2 \sum (y_2 - \pi_1 x_1 - \pi_2 x_2) x_2 = 0
$$

這些為 **最小平方法 (OLS)** 條件，用以求解  π^1,  π^2 的常見正規方程式（normal equations）。

----

**(f) 插入數值求解**

我們根據 OLS 一階條件式得到以下 moment conditions：

$$
\frac{1}{N} \sum x_{i1} (y_2 - \pi_1 x_{i1} - \pi_2 x_{i2}) = 0
$$

$$
\frac{1}{N} \sum x_{i2} (y_2 - \pi_1 x_{i1} - \pi_2 x_{i2}) = 0
$$

展開後得到：

$$
\sum x_{i1} y_{i2} - \pi_1 \sum x_{i1}^2 - \pi_2 \sum x_{i1} x_{i2} = 0
$$

$$
\sum x_{i2} y_{i2} - \pi_1 \sum x_{i1} x_{i2} - \pi_2 \sum x_{i2}^2 = 0
$$

插入已知值計算：

$$
3 - \hat{\pi}_1 * 1 - \pi_2 * 0 = 0 \quad \Rightarrow \quad \hat{\pi}_1 = 3
$$

$$
4 - \hat{\pi}_1 * 0 - \hat{\pi}_2 * 1 = 0 \quad \Rightarrow \quad \hat{\pi}_2 = 4
$$

----

**(g) 求 α_1  的 IV 估計量**

已知結構方程式為：

$$
y_1 = \alpha_1 y_2 + e_1
$$

且：

$$
\hat{y}_2 = \hat{\pi}_1 x_1 + \hat{\pi}_2 x_2
$$

根據 moment condition（即 \( E[\hat{y}_2 (y_1 - \alpha_1 y_2)] = 0 \)），可得：

$$
\sum \hat{y}_2 (y_1 - \alpha_1 y_2) = 0 
\Rightarrow 
\sum \hat{y}_2 y_1 - \alpha_1 \sum \hat{y}_2 y_2 = 0 
\Rightarrow 
\alpha_1 = \frac{\sum \hat{y}_2 y_1}{\sum \hat{y}_2 y_2}
$$

代入 

$$
\( \hat{y}_2 = \hat{\pi}_1 x_1 + \hat{\pi}_2 x_2 \)
$$

，得：

<img width="484" alt="g" src="https://github.com/user-attachments/assets/8960a10d-6288-4de8-a801-6902cb45e77f" />


----

**(h) 證明 Two-Stage Least Squares 解與 IV 解相同**

已知：

$$
\hat{\alpha}_{1, \text{2SLS}} = \frac{\sum \hat{y}_2 y_1}{\sum \hat{y}_2^2}
$$

要證明 

$$
\( \hat{\alpha}_{1, \text{2SLS}} = \hat{\alpha}_1 \)
$$

根據 moment condition，我們需證明：

$$
\sum \hat{y}_2^2 = \sum \hat{y}_2 y_2
$$

由以下推導：

$$
\sum \hat{y}_2^2 = \sum \hat{y}_2 (y_2 - \hat{v}_2)
= \sum \hat{y}_2 y_2 - \sum \hat{y}_2 \hat{v}_2
= \sum \hat{y}_2 y_2
$$

因為：

$$
\sum \hat{y}_2 \hat{v}_2 = 0
$$

這是因為在 OLS 中，預測值 y^_2  與誤差項 v^_2  正交，  
亦即「解釋變數與誤差無關」的基本性質。





