## Q11.01

### (a) 化簡 y₂ 為 reduced-form 並表示誤差項

原始結構模型：

$$
y_1 = \alpha_1 y_2 + e_1 \\
y_2 = \alpha_2 y_1 + \beta_1 x_1 + \beta_2 x_2 + e_2
$$

**Step 1：代入 y₁ 表達式進 y₂**

$$
y_1 = \alpha_1 y_2 + e_1 
\Rightarrow y_2 = \alpha_2 (\alpha_1 y_2 + e_1) + \beta_1 x_1 + \beta_2 x_2 + e_2
$$

整理：

$$
y_2 - \alpha_1 \alpha_2 y_2 = \alpha_2 e_1 + \beta_1 x_1 + \beta_2 x_2 + e_2 \\
(1 - \alpha_1 \alpha_2)y_2 = \alpha_2 e_1 + \beta_1 x_1 + \beta_2 x_2 + e_2
$$

**Step 2：化成 reduced-form**

$$
y_2 = \frac{\beta_1}{1 - \alpha_1 \alpha_2} x_1 + \frac{\beta_2}{1 - \alpha_1 \alpha_2} x_2 + \frac{\alpha_2 e_1 + e_2}{1 - \alpha_1 \alpha_2}
$$

所以：

$$
\pi_1 = \frac{\beta_1}{1 - \alpha_1 \alpha_2}, \quad \pi_2 = \frac{\beta_2}{1 - \alpha_1 \alpha_2}, \quad \nu_2 = \frac{\alpha_2 e_1 + e_2}{1 - \alpha_1 \alpha_2}
$$

**說明：為什麼 $y_2$ 與 $e_1$ 有相關？**

因為誤差項：

$$
\nu_2 = \frac{\alpha_2 e_1 + e_2}{1 - \alpha_1 \alpha_2}
$$

包含了 $e_1$，所以：

$$
\text{Cov}(y_2, e_1) = \text{Cov}(\nu_2, e_1) \neq 0
$$

表示 $y_2$ 與 $e_1$ 有相關性。

---

### (b) 哪些方程的參數可以用 OLS 一致估計？

考察模型：

$$
y_1 = \alpha_1 y_2 + e_1 \\
y_2 = \alpha_2 y_1 + \beta_1 x_1 + \beta_2 x_2 + e_2
$$

* 對第一個方程：$y_2$ 是內生變數，和誤差項 $e_1$ 有相關（見 a 小題）。因此用 OLS 估計 $\alpha_1$ 會產生偏誤（不一致）。
* 對第二個方程：$x_1, x_2$ 是外生變數，與誤差項 $e_2$ 無相關。因此用 OLS 估計 $\alpha_2, \beta_1, \beta_2$ 是一致的。

**結論：**

* $\alpha_2, \beta_1, \beta_2$ 可以用 OLS 一致估計。
* $\alpha_1$ 不可以，因為 $y_2$ 是內生變數。

---

### (c) 哪些參數是「識別的」？為什麼？

在聯立方程模型中，識別性的判斷依賴於工具變數的存在與外生變數的排除。

結構方程：

1. $y_1 = \alpha_1 y_2 + e_1$
2. $y_2 = \alpha_2 y_1 + \beta_1 x_1 + \beta_2 x_2 + e_2$

* 第一個方程中，$y_2$ 是內生變數，我們是否有足夠外生變數當作工具變數？是的，因為有兩個外生變數 $x_1, x_2$，但它們沒出現在第一個方程中，可作為工具變數。
* 第二個方程中，$x_1, x_2$ 是外生變數，與誤差項無相關，模型中也包含了足夠的解釋變數。

**結論：**

* 第一個方程（估 $\alpha_1$）是**過度識別（over-identified）**，因為有 2 個工具變數用來估 1 個內生變數。
* 第二個方程中所有參數（$\alpha_2, \beta_1, \beta_2$）是識別的，因為變數都為外生或可觀察。

因此：**所有參數在此模型中都是識別的。**

---

### (d) 為什麼矩估法 (MOM) 可以一致估計 reduced-form 參數？

Reduced-form 方程：

$$
y_2 = \pi_1 x_1 + \pi_2 x_2 + \nu_2
$$

MOM 矩條件：

$$
\frac{1}{N} \sum x_{1i}(y_{2i} - \pi_1 x_{1i} - \pi_2 x_{2i}) = 0 \\
\frac{1}{N} \sum x_{2i}(y_{2i} - \pi_1 x_{1i} - \pi_2 x_{2i}) = 0
$$

條件成立的原因：

* $x_1, x_2$ 是外生變數，與誤差項 $\nu_2$ 無相關。
* 因此這些矩條件在樣本數趨近無限時會收斂到真實值，從而使估計一致。

---

### (e) MOM 與 OLS 是否相同？推導 OLS 與矩條件的關係

對模型：

$$
y_2 = \pi_1 x_1 + \pi_2 x_2 + \nu_2
$$

最小平方法的目標是最小化誤差平方和：

$$
\min_{\pi_1, \pi_2} \sum (y_{2i} - \pi_1 x_{1i} - \pi_2 x_{2i})^2
$$

對 $\pi_1, \pi_2$ 求一階條件：

$$
\frac{\partial}{\partial \pi_1} = -2 \sum x_{1i}(y_{2i} - \pi_1 x_{1i} - \pi_2 x_{2i}) = 0 \\
\frac{\partial}{\partial \pi_2} = -2 \sum x_{2i}(y_{2i} - \pi_1 x_{1i} - \pi_2 x_{2i}) = 0
$$

這與 (d) 中的矩條件完全一致。

**結論：** MOM 在這種線性模型中與 OLS 結果相同。

---

### (f) 利用矩條件求 $\hat{\pi}_1, \hat{\pi}_2$

已知：

$$
\sum x_{1i}^2 = 1,\quad \sum x_{2i}^2 = 1,\quad \sum x_{1i} x_{2i} = 0 \\
\sum x_{1i} y_{2i} = 3,\quad \sum x_{2i} y_{2i} = 4
$$

根據 MOM 條件：

$$
\hat{\pi}_1 = \frac{\sum x_{1i} y_{2i}}{\sum x_{1i}^2} = \frac{3}{1} = 3 \\
\hat{\pi}_2 = \frac{\sum x_{2i} y_{2i}}{\sum x_{2i}^2} = \frac{4}{1} = 4
$$

---

### (g) 利用 $\hat{y}_2$ 一致估計 $\alpha_1$

令：

$$
\hat{y}_2 = \hat{\pi}_1 x_1 + \hat{\pi}_2 x_2 = 3x_1 + 4x_2
$$

欲估 $\alpha_1$，考慮以下矩條件：

$$
\sum x_{2i}(y_{1i} - \alpha_1 y_{2i}) = 0
$$

因為 $x_2$ 與誤差項 $e_1$ 無關，可以作為工具變數來估 $\alpha_1$。

代入：

$$
\sum x_{2i} y_{1i} = \alpha_1 \sum x_{2i} y_{2i} \\
3 = \alpha_1 \cdot 4 \Rightarrow \hat{\alpha}_1 = \frac{3}{4} = 0.75
$$

---

### (h) 2SLS 估計 $\alpha_1$，與 (g) 比較

第一階段：

$$
\hat{y}_2 = 3x_1 + 4x_2
$$

第二階段回歸：

$$
y_1 = \alpha_1 \hat{y}_2 + e_1^*
$$

代入資料：

$$
\sum \hat{y}_{2i}^2 = 9(1) + 24(0) + 16(1) = 25 \\
\sum \hat{y}_{2i} y_{1i} = 3(3) + 4(3) = 9 + 12 = 21 \\
\Rightarrow \hat{\alpha}_1 = \frac{21}{28} = 0.75
$$

與 (g) 的結果一致，確認 2SLS 和 IV 方法在這裡等價。
