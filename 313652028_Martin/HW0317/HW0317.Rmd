---
title: "HW0317"
author: "Yung-Jung Cheng"
date: "2025-03-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(POE5Rdata)
library(ggplot2)
library(gridExtra)
library(tseries) 
library(lmtest)

```

# Q4
The general manager of a large engineering firm wants to know whether the experience of technical artists influences their work quality. A random sample of 50 artists is selected. Using years of work experience (EXPER) and a performance rating (RATING, on a 100-point scale), two models are estimated by least squares. The estimates and standard errors are as follows:
Model 1:
$$\hat{RATING}= 64.289 + 0.990EXPER,\,\,\,\, N = 50,\,\,\,\, R^2 = 0.3793
$$
Model 2:
$$
\hat{RATING}= 39.464 + 15.312 \ln(EXPER),\,\,\, N = 46,\,\,\, R^2 = 0.6414
$$


## Q4(a)
Sketch the fitted values from Model 1 for EXPER = 0 to 30 years.

### Ans
```{r Q4a}
# å®šç¾©ç¶“é©—å¹´æ•¸ç¯„åœ
exper <- 0:30

# æ ¹æ“šæ¨¡å‹ 1 è¨ˆç®—é æ¸¬çš„è¡¨ç¾è©•åˆ†
rating <- 64.289 + 0.990 * exper

# ç¹ªè£½åœ–å½¢
plot(exper, rating, type = 'l', col = 'blue',
     main = "Model 1 Fitted Values",
     xlab = "Years of Experience (EXPER)",
     ylab = "Performance Rating (RATING)",
     lwd = 2)

# æ·»åŠ ç¶²æ ¼
grid(nx = NULL, ny = NULL, col = "gray", lty = "dotted")


```

## Q4(b)
Sketch the fitted values from Model 2 against EXPER = 1 to 30 years. Explain why the four artists with no experience are not used in the estimation of Model 2.

### Ans
```{r Q4b}
# å®šç¾©ç¶“é©—å¹´æ•¸ç¯„åœï¼ˆå¾ 1 åˆ° 30ï¼‰
exper <- 1:30

# æ ¹æ“šæ¨¡å‹ 2 è¨ˆç®—é æ¸¬çš„è¡¨ç¾è©•åˆ†ï¼Œä½¿ç”¨è‡ªç„¶å°æ•¸
rating <- 39.464 + 15.312 * log(exper)

# ç¹ªè£½åœ–å½¢
plot(exper, rating, type = 'l', col = 'green',
     main = "Model 2 Fitted Values",
     xlab = "Years of Experience (EXPER)",
     ylab = "Performance Rating (RATING)",
     lwd = 2)

# æ·»åŠ ç¶²æ ¼
grid(nx = NULL, ny = NULL, col = "gray", lty = "dotted")


```

Since $\ln(0)$ is undefined, we can not use model2 to pridict those four astist with no experience.

## Q4(c)
Using Model 1, compute the marginal effect on RATING of another year of experience for (i) an artist with 10 years of experience and (ii) an artist with 20 years of experience.

### Ans
Marge effect of model 1 is fitted and is 0.990. Also the model is linear, such that marginal effect on RATING of another year of experience for both 10 years of experience and 20 years of experience will be 0.990.


## Q4(d)
Using Model 2, compute the marginal effect on RATING of another year of experience for (i) an artist with 10 years of experience and (ii) an artist with 20 years of experience.

### Ans

$ \frac{\partial\text{RATING}}{\partial\text{EXPER}}=15.312\times\frac{1}{\text{EXPER}}$

1) for a 10 years experience, $15.312\times\frac{1}{10}=1.5312$
2) for a 20 years experience, $15.312\times\frac{1}{20}=0.7656$

## Q4(e)
Which of the two models fits the data better? Estimation of Model 1 using just the technical artists with some experience yields R2 = 0.4858.

### Ans
Model 2 fits the data better, since $0.6414>0.4858$

## Q4(f)
Do you find Model 1 or Model 2 more reasonable, or plausible, based on economic reasoning? Explain.

### Ans
**Model 2 is more reasonable** based on economic reasoning because it accounts for diminishing returns on experience. This model uses the logarithm of experience, reflecting a decrease in the impact of each additional year of experience as an artist becomes more skilled. This approach aligns better with real-world scenarios where improvements in performance tend to be greater at the beginning of a career and diminish over time.

# Q28
The file wa-wheat.dat contains observations on wheat yield in Western Australian shires. There are 48 annual observations for the years 1950â€“1997. For the Northampton shire, consider the following four equations:
$$
\begin{align}
YIELD_t &= Î²_0 + Î²_1 TIME + e_t\\
YIELD_t &= Î±_0 + Î±_1 \ln(TIME) + e_t\\
YIELD_t &= Î³_0 + Î³_1 TIME2 + e_t\\
\ln(YIELD_t) &= Ï•_0 + Ï•_1 TIME + e_t

\end{align}
$$


## Q28(a)
Estimate each of the four equations. Taking into consideration (i) plots of the fitted equations, (ii) plots of the residuals, (iii) error normality tests, and (iii) values for R2, which equation do you think is preferable? Explain.

### Ans
```{r Q28a}
# æ•¸æ“šæº–å‚™
time_data <- wa_wheat$time
yield_data <- wa_wheat$northampton

# æ¨¡å‹æ“¬åˆ
model_linear <- lm(yield_data ~ time_data, data = wa_wheat)
model_log_time <- lm(yield_data ~ log(time_data), data = wa_wheat)
model_quadratic <- lm(yield_data ~ I(time_data^2), data = wa_wheat)
model_log_yield <- lm(log(yield_data) ~ time_data, data = wa_wheat)

# æª¢è¦–æ¨¡å‹æ‘˜è¦ï¼Œç²å–R^2å’Œæ®˜å·®æª¢é©—
summary(model_linear)
summary(model_log_time)
summary(model_quadratic)
summary(model_log_yield)

# æ­£æ…‹æ€§æª¢æ¸¬
shapiro.test(residuals(model_linear))
shapiro.test(residuals(model_log_time))
shapiro.test(residuals(model_quadratic))
shapiro.test(residuals(model_log_yield))

```
Based on the analysis, the **Quadratic Model (Model 3)** is the preferred choice. It has the highest \(R^2\) value of 0.689, indicating it explains the variability in the data better than the other models. The residuals are closest to a normal distribution, as confirmed by the Shapiro-Wilk test (\(p = 0.8266\)), which supports the model's assumptions. Therefore, the quadratic model provides the best fit and most reliable statistical inferences for the data on wheat yield in Northampton.

## Q28(b)
Interpret the coefficient of the time-related variable in your chosen specification.

### Ans
In the chosen Quadratic Model (Model 3), the coefficient of the time-squared variable (\(\beta_1 = 0.0004986\)) represents the accelerating effect of time on wheat yield. Specifically, this coefficient indicates that as time progresses, the rate of increase in wheat yield accelerates. This suggests that wheat yield does not simply increase linearly over time but grows faster as time squares, reflecting potential improvements in technology, farming practices, or environmental factors.

## Q28(c)
Using your chosen specification, identify any unusual observations, based on the studentized residuals, LEVERAGE, DFBETAS, and DFFITS.

### Ans
```{r 28c}
# è¨­å®šæ¨¡å‹
model_quadratic <- lm(northampton ~ I(time^2), data = wa_wheat)

# å­¸ç”ŸåŒ–æ®˜å·®
student_resids <- rstudent(model_quadratic)
leverage_threshold <- 2 * mean(hatvalues(model_quadratic))
dffits_values <- dffits(model_quadratic)
dffits_threshold <- 2 * sqrt(2/nrow(wa_wheat))

# ç¹ªè£½å­¸ç”ŸåŒ–æ®˜å·®
plot(wa_wheat$time, student_resids, ylab="Studentized Residuals", xlab="Time", main="Plot of Studentized Residuals")
abline(h = c(-2, 2), col = "red")
# æ¨™è¨˜ç•°å¸¸å€¼
outliers_resids <- which(abs(student_resids) > 2)
points(wa_wheat$time[outliers_resids], student_resids[outliers_resids], col = "blue", pch = 19)

# ç¹ªè£½æ æ¡¿å€¼
plot(wa_wheat$time, hatvalues(model_quadratic), ylab="Leverages", xlab="Time", main="Plot of Leverages")
abline(h = leverage_threshold, col = "red")
# æ¨™è¨˜ç•°å¸¸å€¼
outliers_leverage <- which(hatvalues(model_quadratic) > leverage_threshold)
points(wa_wheat$time[outliers_leverage], hatvalues(model_quadratic)[outliers_leverage], col = "blue", pch = 19)

# ç¹ªè£½DFFITS
plot(wa_wheat$time, dffits_values, ylab="DFFITS", xlab="Time", main="DFFITS Values")
abline(h = c(-dffits_threshold, dffits_threshold), col = "red")
# æ¨™è¨˜ç•°å¸¸å€¼
outliers_dffits <- which(abs(dffits_values) > dffits_threshold)
points(wa_wheat$time[outliers_dffits], dffits_values[outliers_dffits], col = "blue", pch = 19)

# æ‰“å°å‡ºæ‰€æœ‰ç•°å¸¸å€¼çš„æ™‚é–“å’ŒåŒ—å®‰æ™®æ•¦ç”¢é‡
wa_wheat[unique(c(outliers_resids, outliers_leverage, outliers_dffits)), ]


```


## Q28(d)
Using your chosen specification, use the observations up to 1996 to estimate the model. Construct a 95% prediction interval for YIELD in 1997. Does your interval contain the true value?

### Ans

```{r Q28d}
# éæ¿¾1996å¹´åŠä»¥å‰çš„æ•¸æ“šï¼ˆå¾1950å¹´é–‹å§‹ï¼Œå°æ‡‰åˆ°1996å¹´ç‚º time = 47ï¼‰
data_train <- wa_wheat[wa_wheat$time <= 47, ]

# æ“¬åˆäºŒæ¬¡æ¨¡å‹
model_train <- lm(northampton ~ I(time^2), data = data_train)

# é€²è¡Œ1997å¹´çš„é æ¸¬ï¼ˆå°æ‡‰åˆ° time = 48ï¼‰
data_1997 <- wa_wheat[wa_wheat$time == 48, ]
predict_1997 <- predict(model_train, newdata = data_1997, interval = "prediction", level = 0.95)

# è¼¸å‡ºé æ¸¬çµæœå’Œ95%é æ¸¬å€é–“
print(predict_1997)

# æª¢æŸ¥1997å¹´å¯¦éš›ç”¢é‡æ˜¯å¦åœ¨é æ¸¬å€é–“å…§
actual_1997 <- data_1997$northampton
print(actual_1997)
within_interval <- actual_1997 >= predict_1997[1, "lwr"] & actual_1997 <= predict_1997[1, "upr"]
print(within_interval)

# å»ºç«‹é æ¸¬è³‡æ–™æ¡†
df_pred <- data.frame(
  time = 48,
  fit = predict_1997[1, "fit"],
  lwr = predict_1997[1, "lwr"],
  upr = predict_1997[1, "upr"],
  actual = actual_1997
)

# éå»è§€æ¸¬è³‡æ–™ (1950â€“1996)
plot_data <- wa_wheat[wa_wheat$time <= 47, ]


# ç¹ªåœ–
ggplot() +
  # è§€æ¸¬è³‡æ–™é»
  geom_point(data = plot_data, aes(x = time, y = northampton), color = "black") +
  
  # æ“¬åˆçš„äºŒæ¬¡æ›²ç·š
  geom_smooth(data = plot_data, aes(x = time, y = northampton), method = "lm",
              formula = y ~ I(x^2), se = FALSE, color = "blue") +
  
  # 1997 é æ¸¬é»
  geom_point(data = df_pred, aes(x = time, y = fit), color = "blue", shape = 17, size = 3) +
  
  # 95% é æ¸¬å€é–“
  geom_errorbar(data = df_pred, aes(x = time, ymin = lwr, ymax = upr), width = 0.5, color = "blue") +
  
  # å¯¦éš›è§€æ¸¬å€¼
  geom_point(data = df_pred, aes(x = time, y = actual), color = "red", shape = 19, size = 3) +
  
  labs(title = "Prediction for 1997 with 95% Prediction Interval",
       x = "Time (1950 = 1, ..., 1997 = 48)",
       y = "Yield (Northampton)") +
  theme_minimal()

```

**Yes, the interval contain true value**


# Q29
Consider a model for household expenditure as a function of household income using the 2013 data from the Consumer Expenditure Survey, cex5_small. The data file cex5 contains more observations. Our attention is restricted to three-person households, consisting of a husband, a wife, plus one other. In this exercise, we examine expenditures on a staple item, food. In this extended example, you are asked to compare the linear, log-log, and linear-log specifications.

## Q29(a)
Calculate summary statistics for the variables: FOOD and INCOME. Report for each the sample mean, median, minimum, maximum, and standard deviation. Construct histograms for both variables. Locate the variable mean and median on each histogram. Are the histograms symmetrical and â€œbell-shapedâ€ curves? Is the sample mean larger than the median, or vice versa? Carry out the Jarqueâ€“Bera test for the normality of each variable.

### Ans
```{r Q29a}
library(ggplot2)
library(tseries)  # Jarque-Bera test ç”¨

# æŠ½å–è®Šæ•¸
food <- cex5_small$food
income <- cex5_small$income

# ===== æ•˜è¿°çµ±è¨ˆ =====
food_stats <- c(
  mean = mean(food),
  median = median(food),
  min = min(food),
  max = max(food),
  sd = sd(food)
)

income_stats <- c(
  mean = mean(income),
  median = median(income),
  min = min(income),
  max = max(income),
  sd = sd(income)
)

print(round(food_stats, 4))
print(round(income_stats, 4))

# ===== ç¹ªè£½ç›´æ–¹åœ– =====

# food histogram
ggplot(cex5_small, aes(x = food)) +
  geom_histogram(binwidth = 20, fill = "lightblue", color = "black") +
  geom_vline(aes(xintercept = mean(food)), color = "blue", linetype = "dashed", linewidth = 1) +
  geom_vline(aes(xintercept = median(food)), color = "red", linetype = "dotted", linewidth = 1) +
  labs(title = "Histogram of FOOD", x = "FOOD", y = "Frequency")

# income histogram
ggplot(cex5_small, aes(x = income)) +
  geom_histogram(binwidth = 10, fill = "lightgreen", color = "black") +
  geom_vline(aes(xintercept = mean(income)), color = "blue", linetype = "dashed", linewidth = 1) +
  geom_vline(aes(xintercept = median(income)), color = "red", linetype = "dotted", linewidth = 1) +
  labs(title = "Histogram of INCOME", x = "INCOME", y = "Frequency")

# ===== Jarque-Bera å¸¸æ…‹æ€§æª¢å®š =====
jb_food <- jarque.bera.test(food)
jb_income <- jarque.bera.test(income)

print(jb_food)
print(jb_income)


```

#### Summary Statistics

- **FOOD**:  
  - Mean = 114.44  
  - Median = 99.80  
  - Minimum = 9.63  
  - Maximum = 476.67  
  - Standard Deviation = 72.66  

- **INCOME**:  
  - Mean = 72.14  
  - Median = 65.29  
  - Minimum = 10.00  
  - Maximum = 200.00  
  - Standard Deviation = 41.65  

---

#### Histograms

- Both histograms are **right-skewed**, not symmetrical or bell-shaped.  
- In both cases, the **mean is greater than the median**.

---

#### Jarqueâ€“Bera Normality Test

- **FOOD**:  
  - JB = 648.65, *p* < 2.2e-16 â†’ **Not normally distributed**

- **INCOME**:  
  - JB = 148.21, *p* < 2.2e-16 â†’ **Not normally distributed**

## Q29(b)

Estimate the linear relationship $FOOD = Î²_1 + Î²_2 INCOME + e$. Create a scatter plot FOOD versus INCOME and include the fitted least squares line. Construct a 95% interval estimate for $Î²_2$. Have we estimated the effect of changing income on average FOOD relatively precisely, or not?


### Ans
```{r Q29b}
# å»ºç«‹ç·šæ€§æ¨¡å‹
model_lm <- lm(food ~ income, data = cex5_small)

# é¡¯ç¤ºæ¨¡å‹æ‘˜è¦ï¼ˆåŒ…å«ä¼°è¨ˆå€¼ã€t å€¼èˆ‡ p å€¼ï¼‰
summary(model_lm)

# ç•«å‡ºæ•£ä½ˆåœ–èˆ‡å›æ­¸ç·š
ggplot(cex5_small, aes(x = income, y = food)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE , color = "blue") +
  labs(title = "Scatter Plot of FOOD vs INCOME",
       x = "INCOME", y = "FOOD")

# å»ºç«‹ Î²2 çš„ 95% ä¿¡è³´å€é–“
confint(model_lm, level = 0.95)

```


We estimated the model:  
$$
FOOD = \beta_1 + \beta_2 \cdot INCOME + e
$$

**Estimated regression equation:**
$$
\widehat{FOOD} = 88.5665 + 0.3587 \cdot INCOME
$$

**95% Confidence Interval for** $\beta_2$:

- [0.2619, 0.4555]

**Interpretation:**

- On average, for each additional unit of income, food expenditure increases by **approximately 0.36 units**.
- Since the confidence interval does **not include 0** and the p-value is very small (*p* < 0.0001), income has a **statistically significant** positive effect on food spending.
- The narrow confidence interval indicates a **precise estimate** of the slope.

**Scatter plot observation:**

- The confidence band around the regression line shows the uncertainty in estimating the **mean** response.
- It is expected that many actual data points fall outside this band, since it is **not a prediction interval**.
- Therefore, the presence of many points outside the band does **not imply a poor fit**.


## Q29 (c)
Obtain the least squares residuals from the regression in (b) and plot them against INCOME. Do you observe any patterns? Construct a residual histogram and carry out the Jarqueâ€“Bera test for normality. Is it more important for the variables FOOD and INCOME to be normally distributed, or that the random error $e$ be normally distributed? Explain your reasoning.

### Ans

```{r Q29c}
# æå–æ®˜å·®
residuals_lm <- residuals(model_lm)

# æ®˜å·® vs INCOME
ggplot(data = cex5_small, aes(x = income, y = residuals_lm)) +
  geom_point(alpha = 0.6) +
  geom_hline(yintercept = 0, color = "blue", linetype = "dashed") +
  labs(title = "Residuals vs INCOME", x = "INCOME", y = "Residuals")

# æ®˜å·®ç›´æ–¹åœ–
ggplot(data.frame(residuals = residuals_lm), aes(x = residuals)) +
  geom_histogram(binwidth = 20, fill = "lightgray", color = "black") +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Frequency")

# Jarqueâ€“Bera test for residuals
jarque.bera.test(residuals_lm)


# åŸ·è¡Œ Breusch-Pagan test
bptest(model_lm)

```
**Breuschâ€“Pagan Test for Heteroskedasticity:**

- BP statistic = 14.086  
- Degrees of freedom = 1  
- p-value = 0.00017

**Conclusion:**

- We reject the null hypothesis of constant variance.
- There is statistical evidence of **heteroskedasticity** in the residuals.
- This supports the earlier visual observation that residual spread increases with income.
- As a result, standard errors from the OLS regression may be **biased**, and inference should be interpreted with caution or corrected using robust methods.



## Q29(d)
Calculate both a point estimate and a 95% interval estimate of the elasticity of food expenditure with respect to income at INCOME = 19, 65, and 160, and the corresponding points on the fitted line, which you may treat as not random. Are the estimated elasticities similar or dissimilar? Do the interval estimates overlap or not? As INCOME increases should the income elasticity for food increase or decrease, based on Economics principles?

### Ans
```{r Q29d}
# ç›®æ¨™é»
income_points <- c(19, 65, 160)

# é æ¸¬ foodï¼ˆå›æ­¸ç·šä¸Šçš„é»ï¼Œä¸æ˜¯éš¨æ©Ÿï¼‰
food_hat <- predict(model_lm, newdata = data.frame(income = income_points))

# æ–œç‡ä¼°è¨ˆå€¼èˆ‡æ¨™æº–èª¤
b2 <- coef(model_lm)["income"]
se_b2 <- summary(model_lm)$coefficients["income", "Std. Error"]

# å»ºç«‹å½ˆæ€§é»ä¼°è¨ˆ
elasticity_point <- b2 * income_points / food_hat

# delta method approximation for SE of elasticity
# Var(E) â‰ˆ (INCOME / FOOD)^2 * Var(b2)
elasticity_se <- (income_points / food_hat)^2 * se_b2^2
elasticity_ci_lower <- elasticity_point - 1.96 * sqrt(elasticity_se)
elasticity_ci_upper <- elasticity_point + 1.96 * sqrt(elasticity_se)

# çµæœæ•´ç†
elasticity_table <- data.frame(
  INCOME = income_points,
  FOOD_hat = round(food_hat, 4),
  Elasticity = round(elasticity_point, 4),
  Lower_95CI = round(elasticity_ci_lower, 4),
  Upper_95CI = round(elasticity_ci_upper, 4)
)

print(elasticity_table)

```

We computed the point and 95% confidence interval estimates of the elasticity of food expenditure at different income levels, based on the linear model:

$$
\text{Elasticity} = \hat{\beta}_2 \cdot \frac{INCOME}{\widehat{FOOD}}
$$

| INCOME | Predicted FOOD | Elasticity | 95% CI              |
|--------|----------------|------------|---------------------|
| 19     | 95.3815        | 0.0715     | [0.0522, 0.0907]    |
| 65     | 111.8811       | 0.2084     | [0.1522, 0.2645]    |
| 160    | 145.9564       | 0.3932     | [0.2872, 0.4992]    |

**Observations:**

- Elasticity increases with income, indicating greater responsiveness of food expenditure at higher income levels.
- The confidence intervals do not significantly overlap, suggesting the differences are statistically meaningful.
- All elasticity values are below 1, consistent with food being a **necessity good** in economic theory.



## Q29(e)
For expenditures on food, estimate the log-log relationship $\ln(FOOD)=Î³_1 + Î³_2\ln(INCOME) + e$. Create a scatter plot for $\ln(FOOD)$ versus $\ln(INCOME)$ and include the fitted least squares line. Compare this to the plot in (b). Is the relationship more or less well-defined for the log-log model relative to the linear specification? Calculate the generalized R2 for the log-log model and compare it to the R2 from the linear model. Which of the models seems to fit the data better?

### Ans
```{r Q29e}
# å– log è®Šæ•¸
cex5_small$log_food <- log(cex5_small$food)
cex5_small$log_income <- log(cex5_small$income)

# å»ºç«‹ log-log æ¨¡å‹
model_loglog <- lm(log_food ~ log_income, data = cex5_small)

# é¡¯ç¤ºæ¨¡å‹æ‘˜è¦
summary(model_loglog)

# ç•«åœ–ï¼šln(FOOD) vs ln(INCOME)
ggplot(cex5_small, aes(x = log_income, y = log_food)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE, color = "blue", fill = "lightblue") +
  labs(title = "Scatter Plot of log(FOOD) vs log(INCOME)",
       x = "log(INCOME)", y = "log(FOOD)")



```


We estimated the model:
$$
\ln(FOOD) = \gamma_1 + \gamma_2 \cdot \ln(INCOME) + e
$$

- Estimated elasticity ($\gamma_2$): **0.1863**  
- RÂ² = **0.0332**, slightly lower than the linear model RÂ² = **0.0423**

**Visual comparison:**

- The log-log scatter plot shows a clearer linear trend.
- Points are more symmetrically distributed around the line.

**Conclusion:**

- The log-log model gives a more well-defined visual relationship.
- However, it explains slightly **less variation** in the data than the linear model.

## Q29(f)
Construct a point and 95% interval estimate of the elasticity for the log-log model. Is the elasticity of food expenditure from the log-log model similar to that in part (d), or dissimilar? Provide statistical evidence for your claim.

### Ans
```{r Q29f}
# æå–å½ˆæ€§èˆ‡æ¨™æº–èª¤
gamma2 <- coef(model_loglog)["log_income"]
se_gamma2 <- summary(model_loglog)$coefficients["log_income", "Std. Error"]

# å»ºç«‹ 95% ä¿¡è³´å€é–“
gamma2_ci <- c(
  lower = gamma2 - 1.96 * se_gamma2,
  upper = gamma2 + 1.96 * se_gamma2
)

# é¡¯ç¤ºçµæœ
round(c(Elasticity = gamma2, gamma2_ci), 4)


```


- Estimated elasticity ($\gamma_2$) = **0.1863**
- 95% confidence interval: **[0.1294, 0.2432]**

**Comparison with part (d):**

| INCOME | Elasticity (d) | 95% CI (d)         | Inside log-log CI? |
|--------|----------------|--------------------|---------------------|
| 19     | 0.0715         | [0.0522, 0.0907]   | âŒ No               |
| 65     | 0.2084         | [0.1522, 0.2645]   | âœ… Yes              |
| 160    | 0.3932         | [0.2872, 0.4992]   | âŒ No               |

**Conclusion:**

- The elasticity from the log-log model is close to that at income = 65 in part (d), but differs from those at income = 19 and 160.
- This shows that the log-log model assumes a constant elasticity, while the linear model reveals that elasticity **increases with income**, which may better reflect economic behavior.

## Q29(g)
Obtain the least squares residuals from the log-log model and plot them against ln(INCOME). Do you observe any patterns? Construct a residual histogram and carry out the Jarqueâ€“Bera test for normality. What do you conclude about the normality of the regression errors in this model?

### Ans
```{r Q29g}
# å–å‡º log-log æ¨¡å‹çš„æ®˜å·®
residuals_loglog <- residuals(model_loglog)

# æ®˜å·® vs ln(INCOME)
ggplot(cex5_small, aes(x = log_income, y = residuals_loglog)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "blue", linetype = "dashed") +
  labs(title = "Residuals vs log(INCOME)",
       x = "log(INCOME)", y = "Residuals")

# æ®˜å·®ç›´æ–¹åœ–
ggplot(data.frame(residuals = residuals_loglog), aes(x = residuals)) +
  geom_histogram(binwidth = 0.25, fill = "lightgray", color = "black") +
  labs(title = "Histogram of Residuals (log-log model)",
       x = "Residuals", y = "Frequency")

# Jarqueâ€“Bera æª¢å®š
jarque.bera.test(residuals_loglog)

```

**Residuals vs log(INCOME):**

- The residuals are randomly scattered around zero, with no clear pattern.
- No evidence of heteroskedasticity or model misfit.

**Histogram of residuals:**

- Roughly bell-shaped, slightly left-skewed.
- More symmetric than in the linear model.

**Jarqueâ€“Bera test:**

- JB = 25.85, p-value = 2.44e-06 â†’ Reject normality.
- However, this is a **major improvement** compared to the linear model (JB = 624).

**Conclusion:**

- The residuals from the log-log model are **not perfectly normal**, but much closer to normality.
- The log-log model shows a better error structure and improves model assumptions.


## Q29(h)
For expenditures on food, estimate the linear-log relationship $FOOD = Î±_1 + Î±_2\ln(INCOME) + e$. Create a scatter plot for $FOOD$ versus $\ln(INCOME)$ and include the fitted least squares line. Compare this to the plots in (b) and (e). Is this relationship more well-defined compared to the others? Compare the R2 values. Which of the models seems to fit the data better?

### Ans
```{r Q29h}
# å»ºç«‹ linear-log æ¨¡å‹
model_linlog <- lm(food ~ log_income, data = cex5_small)

# é¡¯ç¤ºæ¨¡å‹æ‘˜è¦ï¼ˆå« RÂ²ï¼‰
summary(model_linlog)

# ç•«å‡º FOOD vs log(INCOME)
ggplot(cex5_small, aes(x = log_income, y = food)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE, color = "blue", fill = "lightblue") +
  labs(title = "Scatter Plot of FOOD vs log(INCOME)",
       x = "log(INCOME)", y = "FOOD")

```


We estimated the model:
$$
FOOD = \alpha_1 + \alpha_2 \cdot \log(INCOME) + e
$$

**Estimated coefficients:**

- Intercept = 23.568  
- Slope = 22.187  
- RÂ² = 0.038  

**Visual comparison:**

- The scatter plot shows a clearer trend than the linear model in (b), but still with substantial spread.
- It is more linear than the original model but less symmetric than the log-log model in (e).

**Conclusion:**

- The linear-log model slightly improves over the log-log model in terms of RÂ² (0.038 vs 0.0332).
- However, it still performs slightly worse than the original linear model (RÂ² = 0.0423).
- None of the models fit particularly well, but each has different strengths.

## Q29(i)
Construct a point and 95% interval estimate of the elasticity for the linear-log model at INCOME = 19, 65, and 160, and the corresponding points on the fitted line, which you may treat as not random. Is the elasticity of food expenditure similar to those from the other models, or dissimilar? Provide statistical evidence for your claim.

### Ans
```{r Q29i}
# è³‡æ–™é»
income_vals <- c(19, 65, 160)

# é æ¸¬å°æ‡‰çš„ foodï¼ˆæ¨¡å‹ fitted å€¼ï¼‰
food_hat_linlog <- predict(model_linlog, newdata = data.frame(log_income = log(income_vals)))

# ä¿‚æ•¸èˆ‡æ¨™æº–èª¤
alpha2 <- coef(model_linlog)["log_income"]
se_alpha2 <- summary(model_linlog)$coefficients["log_income", "Std. Error"]

# å½ˆæ€§ä¼°è¨ˆèˆ‡ä¿¡è³´å€é–“ï¼ˆDelta methodï¼‰
elasticity_point <- (alpha2 / food_hat_linlog) * income_vals
elasticity_se <- ((income_vals / food_hat_linlog)^2) * se_alpha2^2
elasticity_ci_lower <- elasticity_point - 1.96 * sqrt(elasticity_se)
elasticity_ci_upper <- elasticity_point + 1.96 * sqrt(elasticity_se)

# çµæœæ•´ç†
elasticity_table_i <- data.frame(
  INCOME = income_vals,
  FOOD_hat = round(food_hat_linlog, 4),
  Elasticity = round(elasticity_point, 4),
  Lower_95CI = round(elasticity_ci_lower, 4),
  Upper_95CI = round(elasticity_ci_upper, 4)
)

print(elasticity_table_i)

```


We estimated elasticity at three income levels using:
$$
\text{Elasticity} = \frac{\alpha_2}{\widehat{FOOD}} \cdot INCOME
$$

| INCOME | Predicted FOOD | Elasticity | 95% CI               |
|--------|----------------|------------|----------------------|
| 19     | 88.8979        | 4.7421     | [3.3910, 6.0932]     |
| 65     | 116.1872       | 12.4126    | [8.8760, 15.9491]    |
| 160    | 136.1733       | 26.0696    | [18.6418, 33.4974]   |

**Comparison:**

- Elasticity estimates are **much larger** than those from the linear and log-log models.
- Confidence intervals do **not overlap**, indicating statistical difference.
- This suggests the linear-log model implies **extremely high elasticity**, which is **unrealistic** for food expenditure.

**Conclusion:**

- The elasticity estimates from the linear-log model are statistically and economically **dissimilar** from the others.
- This model may **overstate the responsiveness** of food expenditure to income.


## Q29(j)
Obtain the least squares residuals from the linear-log model and plot them against ln(INCOME). Do you observe any patterns? Construct a residual histogram and carry out the Jarqueâ€“Bera test for normality. What do you conclude about the normality of the regression errors in this model?

### Ans
```{r Q29j}
# å–å¾—æ®˜å·®
residuals_linlog <- residuals(model_linlog)

# æ®˜å·® vs ln(INCOME)
ggplot(cex5_small, aes(x = log_income, y = residuals_linlog)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "blue", linetype = "dashed") +
  labs(title = "Residuals vs log(INCOME) (Linear-Log Model)",
       x = "log(INCOME)", y = "Residuals")

# æ®˜å·®ç›´æ–¹åœ–
ggplot(data.frame(residuals = residuals_linlog), aes(x = residuals)) +
  geom_histogram(binwidth = 20, fill = "lightgray", color = "black") +
  labs(title = "Histogram of Residuals (Linear-Log Model)",
       x = "Residuals", y = "Frequency")

# Jarqueâ€“Bera test
jarque.bera.test(residuals_linlog)

```

**Residuals vs log(INCOME):**

- The residuals show a right-skewed pattern with increasing spread at higher income levels.
- Suggests some heteroskedasticity.

**Histogram of residuals:**

- Strongly right-skewed, not bell-shaped.
- Indicates departure from normality.

**Jarqueâ€“Bera test:**

- JB = 628.07, p-value < 2.2e-16 â†’ Strong rejection of normality.

**Conclusion:**

- The residuals from the linear-log model are **not normally distributed**.
- The model violates the normality assumption, and the error structure is not well-behaved.
- Compared to the log-log model, this one performs **worse** in terms of residual normality.


## Q29(k)
Based on this exercise, do you prefer the linear relationship model, or the log-log model or the linear-log model? Explain your reasoning.

### Ans

After comparing all three models, I choose the **log-log model**:
$$
\log(FOOD) = \gamma_1 + \gamma_2 \cdot \log(INCOME) + e
$$

Although its RÂ² is slightly lower than the linear model, the log-log model has the **best residual behavior**. The residuals are much closer to normal, show no strong pattern, and the Jarqueâ€“Bera test statistic is far lower than for the other models.

#### ğŸ“Š Comparison of Models

| Model        | Equation                                     | Elasticity        | RÂ²      | JB Test (p-value)      | Residual Shape        |
|--------------|----------------------------------------------|-------------------|---------|-------------------------|------------------------|
| **Linear**   | $FOOD = \beta_1 + \beta_2 \cdot INCOME$      | Varies (small)    | 0.0423  | 624.19 (p < 2e-16) â›”ï¸   | Right-skewed, wide     |
| **Log-Log**  | $\log(FOOD) = \gamma_1 + \gamma_2 \cdot \log(INCOME)$ | Constant â‰ˆ 0.186 | 0.0332  | 25.85 (p = 2e-6) âœ…     | Symmetric, centered    |
| **Linear-Log** | $FOOD = \alpha_1 + \alpha_2 \cdot \log(INCOME)$ | Varies (too large) | 0.0380  | 628.07 (p < 2e-16) â›”ï¸   | Strongly right-skewed  |

---

### âœ… Conclusion:

- The **log-log model** provides the best overall trade-off:
  - It has **stable, approximately normal residuals**.
  - Its elasticity estimate is constant and **economically plausible** (below 1).
- Despite a lower RÂ², the **model assumptions are better satisfied**.
- Therefore, I select the log-log specification as the most appropriate model for analyzing food expenditure.














