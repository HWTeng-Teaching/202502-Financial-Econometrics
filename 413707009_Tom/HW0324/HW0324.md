
---
title: "HW0324"
author: "葛同 (413707008)"
date: "2025-03-30"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Q1

# Derivation: Transitioning Matrix-Form OLS to Standard Simple Linear Regression Formulas (2.7)–(2.8)

## 1) Introduction: Simple Linear Regression Framework

Given a set of $n$ observations $\{(x_i, y_i)\}_{i=1}^n$, we aim to estimate the model:
$$ y_i = eta_1 + eta_2 x_i + u_i. $$

In matrix notation, the model becomes:

$$
\mathbf{Y} = 
egin{pmatrix}
y_1 \[4pt]
y_2 \[2pt]
dots \[2pt]
y_n
\end{pmatrix},
\quad
\mathbf{X} = 
egin{pmatrix}
1 & x_1 \ 
1 & x_2 \ 
dots & dots \ 
1 & x_n
\end{pmatrix},
\quad
oldsymbol{eta} = 
egin{pmatrix}
eta_1 \[3pt]
eta_2
\end{pmatrix},
\quad
\mathbf{u} = 
egin{pmatrix}
u_1 \[3pt]
u_2 \ 
dots \ 
u_n
\end{pmatrix}
$$

The ordinary least squares (OLS) estimator is expressed as:
$$ \mathbf{b} = (\mathbf{X}^	op\mathbf{X})^{-1}(\mathbf{X}^	op\mathbf{Y}) $$

The goal is to demonstrate that this matrix-based formula collapses to the traditional formulas (2.7)–(2.8) for simple linear regression:

$$
b_2 = rac{\sum_{i=1}^n (x_i - ar{x})(y_i - ar{y})}{\sum_{i=1}^n (x_i - ar{x})^2}, \quad
b_1 = ar{y} - b_2ar{x}
$$

where $ar{x} = rac{1}{n} \sum x_i$ and $ar{y} = rac{1}{n} \sum y_i$.

## 2) Calculate $\mathbf{X}^	op \mathbf{X}$ and its Inverse

Start by computing $\mathbf{X}^	op \mathbf{X}$:

$$
\mathbf{X}^	op \mathbf{X}
= 
egin{pmatrix}
1 & 1 & \dots & 1 \
x_1 & x_2 & \dots & x_n
\end{pmatrix}
egin{pmatrix}
1 & x_1 \
1 & x_2 \
dots & dots \
1 & x_n
\end{pmatrix}
= 
egin{pmatrix}
\sum_{i=1}^n 1 & \sum_{i=1}^n x_i \
\sum_{i=1}^n x_i & \sum_{i=1}^n x_i^2
\end{pmatrix}
= 
egin{pmatrix}
n & \sum x_i \
\sum x_i & \sum x_i^2
\end{pmatrix}
$$

To find the inverse of this $2 	imes 2$ matrix, we use the standard formula:

$$
(\mathbf{X}^	op \mathbf{X})^{-1}
= 
rac{1}{n \sum x_i^2 - (\sum x_i)^2}
egin{pmatrix}
\sum x_i^2 & -\sum x_i \
-\sum x_i & n
\end{pmatrix}
$$

Let $D = n \sum x_i^2 - (\sum x_i)^2$ for convenience.

## 3) Calculate $\mathbf{X}^	op \mathbf{Y}$

Now, compute $\mathbf{X}^	op \mathbf{Y}$:

$$
\mathbf{X}^	op \mathbf{Y}
=
egin{pmatrix}
1 & 1 & \cdots & 1 \
x_1 & x_2 & \cdots & x_n
\end{pmatrix}
egin{pmatrix}
y_1 \
y_2 \
dots \
y_n
\end{pmatrix}
=
egin{pmatrix}
\sum_{i=1}^n y_i \
\sum_{i=1}^n x_i y_i
\end{pmatrix}
$$

## 4) Calculate $\mathbf{b} = (\mathbf{X}^	op \mathbf{X})^{-1}(\mathbf{X}^	op \mathbf{Y})$

Multiply the inverse of $\mathbf{X}^	op \mathbf{X}$ by $\mathbf{X}^	op \mathbf{Y}$:

$$
egin{pmatrix}
b_1 \
b_2
\end{pmatrix}
=
rac{1}{D}
egin{pmatrix}
\sum x_i^2 & -\sum x_i \
-\sum x_i & n
\end{pmatrix}
egin{pmatrix}
\sum y_i \
\sum x_i y_i
\end{pmatrix}
$$

This yields:

$$
b_1 = rac{1}{D} \left[ (\sum x_i^2)(\sum y_i) - (\sum x_i)(\sum x_i y_i) ight]
$$

$$
b_2 = rac{1}{D} \left[ - (\sum x_i)(\sum y_i) + n(\sum x_i y_i) ight]
$$

## 5) Express in Terms of Mean Deviations

Next, focus on simplifying $b_2$:

$$
b_2 = rac{n(\sum x_i y_i) - (\sum x_i)(\sum y_i)}{n(\sum x_i^2) - (\sum x_i)^2}
$$

Using identities for the sample means:

- $ar{x} = rac{1}{n} \sum x_i$ so $\sum x_i = nar{x}$
- $ar{y} = rac{1}{n} \sum y_i$ so $\sum y_i = nar{y}$

We can rewrite:

$$
b_2 = rac{n(\sum x_i y_i) - n^2 ar{x} ar{y}}{n(\sum x_i^2) - n^2 ar{x}^2} = rac{\sum x_i y_i - nar{x}ar{y}}{\sum x_i^2 - nar{x}^2}
$$

By applying standard identities for the mean deviations:

1. $\sum (x_i - ar{x})(y_i - ar{y}) = \sum x_i y_i - nar{x}ar{y}$ 
2. $\sum (x_i - ar{x})^2 = \sum x_i^2 - nar{x}^2$

This gives:

$$
b_2 = rac{\sum (x_i - ar{x})(y_i - ar{y})}{\sum (x_i - ar{x})^2}
$$

This matches formula (2.7).

Finally, $b_1$ becomes:

$$
b_1 = ar{y} - b_2 ar{x}
$$

Which matches formula (2.8).

## Conclusion

We’ve shown that the matrix-form OLS estimator $\mathbf{b} = (\mathbf{X}^	op \mathbf{X})^{-1} (\mathbf{X}^	op \mathbf{Y})$ simplifies to the familiar formulas for simple linear regression:

$$
b_2 = rac{\sum_{i=1}^n (x_i - ar{x})(y_i - ar{y})}{\sum_{i=1}^n (x_i - ar{x})^2}, \quad
b_1 = ar{y} - b_2 ar{x}
$$

Thus, both approaches yield the same estimators for the simple linear regression model.
